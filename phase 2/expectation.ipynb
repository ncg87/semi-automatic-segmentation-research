{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General use for DL\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# To loader dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Measurements\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics import Dice\n",
    "\n",
    "# Dataset import\n",
    "from dataset import ISICSegmentationDataset\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from exp_utils import stopping_rule, moving_avg\n",
    "from train import train_model, test_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to masks and images\n",
    "image_path = \"./ISIC/images/ISIC2018_Task1-2_Training_Input/\"\n",
    "masks_path = \"./ISIC/masks/ISIC2018_Task1_Training_GroundTruth/\"\n",
    "# Size of image\n",
    "size = 256\n",
    "# Define dataset\n",
    "dataset = ISICSegmentationDataset(image_path, masks_path, size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminary variables ##\n",
    "\n",
    "# Specifies whether to train on GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loss for training\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Measurements\n",
    "jaccard = JaccardIndex(task='multiclass', num_classes = 2, average = 'micro').to(device)\n",
    "dice = Dice(num_classes = 2, average = 'micro').to(device)\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Stopping threshold\n",
    "threshold = 0.001\n",
    "\n",
    "# Speeds up training\n",
    "num_workers = 8\n",
    "\n",
    "# Alpha for EMA\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Test Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File name\n",
    "name = 'baseline.pt'\n",
    "baseline = torch.load(f=name)\n",
    "# Extract indices and baseline jaccard and dice scores\n",
    "all_indices = baseline['fold_dict']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['A', 'B', 'C', 'D', 'E']\n",
    "split_dices = {}\n",
    "for split in splits:\n",
    "    # Get indices of test and train points in dataset\n",
    "    indices = all_indices[split]\n",
    "    train_indices = indices[0]\n",
    "    test_indices = indices[1]\n",
    "    \n",
    "    # Array to store each splits dices\n",
    "    test_dice = []\n",
    "    \n",
    "    # Create test dataloader for fold\n",
    "    test_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_indices),\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    \n",
    "    # Array to store indices of model, increased by 10 at a time until we get to the whole train set\n",
    "    model_indices = []\n",
    "    \n",
    "    while len(train_indices) > 0:\n",
    "        # To prevent selection of more points than available, causing error\n",
    "        n_to_select = min(10, len(train_indices))\n",
    "        # Get a random 10 point of the train set\n",
    "        model_indices.extend(np.random.choice(train_indices, n_to_select, replace=False))\n",
    "        # Remove the indices from the the next round of selection\n",
    "        train_indices = np.setdiff1d(train_indices, model_indices) \n",
    "        # Create a train dataloader for partition\n",
    "        train_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(model_indices),\n",
    "            num_workers = num_workers\n",
    "        )\n",
    "        \n",
    "        ## Initialize the model ##\n",
    "\n",
    "        # Loading an untrained model to GPU/CPU\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "            in_channels=3, out_channels=1, init_features=64, pretrained=False, trust_repo=True).to(device)\n",
    "        # We will begin our learning rate at 0.01 \n",
    "        lr = 0.01\n",
    "        # Optimizer for model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,25)\n",
    "        \n",
    "        ## Initialize the training ##\n",
    "\n",
    "        # Initialize previous and current loss for stopping rule\n",
    "        \n",
    "        L_MA = 1 # Moving average of loss, intialized to zero for error purposes\n",
    "        L_k = 0 # Current loss\n",
    "        \n",
    "        # To determine in threshold is too low\n",
    "        counter = 0\n",
    "        # Train model until stopping rule is reached\n",
    "        while(stopping_rule(L_MA, L_k, threshold) or counter < 10):\n",
    "            # Train model and compute loss\n",
    "            L_k = train_model(model, loss_fn, device, train_loader, optimizer)\n",
    "            \n",
    "            # Initialization of EMA\n",
    "            if(L_MA == 0):\n",
    "                L_MA = L_k\n",
    "            \n",
    "            # Find EMA of losses\n",
    "            L_MA = moving_avg(alpha, L_MA, L_k)\n",
    "        # Get the dice score of the trained model\n",
    "        jaccard_score, dice_score = test_model(model, device, test_loader, jaccard, dice)\n",
    "        print(f\"Model split {split}, {len(model_indices)} data points | Dice Score: {dice_score}\")\n",
    "        # Store the dice score\n",
    "        test_dice.append(dice_score)\n",
    "        \n",
    "    # Store that splits dice scores in dictionary\n",
    "    split_dices[split] = test_dice\n",
    "\n",
    "# Store the dictionary of dice scores\n",
    "torch.save(split_dices, 'split_dices.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chart the dice progression average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the length of a arbitrary split to compute the average dice scores of them \n",
    "average_dices = np.zeros(len(splits[\"A\"]))\n",
    "for split in splits:\n",
    "    average_dices += split_dices[split]\n",
    "average_dices /= len(splits)\n",
    "\n",
    "plt.plot(average_dices, label=\"Average Dice Scores\")\n",
    "plt.xlim(0, 800)\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
