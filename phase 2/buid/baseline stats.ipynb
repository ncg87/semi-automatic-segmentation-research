{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:37:58.504992Z",
     "iopub.status.busy": "2024-10-04T15:37:58.504452Z",
     "iopub.status.idle": "2024-10-04T15:38:05.876838Z",
     "shell.execute_reply": "2024-10-04T15:38:05.876180Z",
     "shell.execute_reply.started": "2024-10-04T15:37:58.504992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting torchmetrics\n",
      "  Downloading torchmetrics-1.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.3)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.1.1+cu121)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.7-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "Downloading torchmetrics-1.4.2-py3-none-any.whl (869 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m869.2/869.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.7-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.7 torchmetrics-1.4.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchmetrics\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Packages required for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:05.877911Z",
     "iopub.status.busy": "2024-10-04T15:38:05.877738Z",
     "iopub.status.idle": "2024-10-04T15:38:11.157768Z",
     "shell.execute_reply": "2024-10-04T15:38:11.157258Z",
     "shell.execute_reply.started": "2024-10-04T15:38:05.877894Z"
    }
   },
   "outputs": [],
   "source": [
    "# General use for DL\n",
    "import torch\n",
    "\n",
    "# Used to split data\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# To loader dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Measurements\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics import Dice\n",
    "\n",
    "# Dataset import\n",
    "from dataset import BUIDSegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping Rule\n",
    "When this function returns a value less than 0.1 % = 0.001 then we know to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.159373Z",
     "iopub.status.busy": "2024-10-04T15:38:11.159111Z",
     "iopub.status.idle": "2024-10-04T15:38:11.161735Z",
     "shell.execute_reply": "2024-10-04T15:38:11.161396Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.159356Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finds percent change between previous and current loss\n",
    "# and if it is less than threshold return false\n",
    "def stopping_rule(L_k, L_k1, threshold):\n",
    "    return abs(L_k - L_k1) / L_k > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.163069Z",
     "iopub.status.busy": "2024-10-04T15:38:11.162791Z",
     "iopub.status.idle": "2024-10-04T15:38:11.165073Z",
     "shell.execute_reply": "2024-10-04T15:38:11.164758Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.163052Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculates the moving average\n",
    "def moving_avg(alpha, L_MA, L_k):\n",
    "    return alpha * L_MA + (1-alpha) * L_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.165813Z",
     "iopub.status.busy": "2024-10-04T15:38:11.165599Z",
     "iopub.status.idle": "2024-10-04T15:38:11.168652Z",
     "shell.execute_reply": "2024-10-04T15:38:11.168338Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.165798Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, device, train_loader, optimizer):\n",
    "    # Initalize loss\n",
    "    average_loss = 0\n",
    "    # Train on dataset\n",
    "    model.train()\n",
    "    for batch_idx, (X,y) in enumerate(train_loader):\n",
    "        # Get batch\n",
    "        image, mask = X.to(device), y.to(device)\n",
    "        # Get results\n",
    "        output = model(image)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, mask)\n",
    "        average_loss += loss.item()\n",
    "        # Optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Return average loss\n",
    "    return average_loss / len(train_loader)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.169396Z",
     "iopub.status.busy": "2024-10-04T15:38:11.169169Z",
     "iopub.status.idle": "2024-10-04T15:38:11.172552Z",
     "shell.execute_reply": "2024-10-04T15:38:11.172239Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.169383Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, device, test_loader, jaccard, dice):\n",
    "    # Initalize average jaccard and dice\n",
    "    average_jaccard = 0\n",
    "    average_dice = 0\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    for batch_idx, (X,y) in enumerate(test_loader):\n",
    "        # Get batch\n",
    "        image, mask = X.to(device), y.to(device)\n",
    "        # Get results\n",
    "        output = model(image)\n",
    "        average_jaccard += jaccard(torch.where(output > 0.5, 1, 0),torch.where(mask > 0.50, 1, 0)).item()\n",
    "        average_dice += dice(torch.where(output > 0.5, 1, 0),torch.where(mask > 0.50, 1, 0)).item()\n",
    "    # Get average of dice and jaccard scores\n",
    "    average_jaccard /= len(test_loader)\n",
    "    average_dice /= len(test_loader)\n",
    "\n",
    "    # Return values\n",
    "    return average_jaccard, average_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "Importing the dataset into the Juypter Notebook enviroment for use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.173249Z",
     "iopub.status.busy": "2024-10-04T15:38:11.173035Z",
     "iopub.status.idle": "2024-10-04T15:38:11.197104Z",
     "shell.execute_reply": "2024-10-04T15:38:11.196736Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.173235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to images and mask\n",
    "root_dir = './BUID'\n",
    "# U-Net we are using takes 3 x 256 x 256 images\n",
    "size = 256\n",
    "# Importing the dataset\n",
    "dataset = BUIDSegmentationDataset(root_dir, size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading preliminary variables for the experiment:\n",
    "- Jaccard score\n",
    "- Dice score\n",
    "- Loss fuction\n",
    "- Device\n",
    "- KFold\n",
    "- Batch size\n",
    "- Stopping threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:11.197921Z",
     "iopub.status.busy": "2024-10-04T15:38:11.197677Z",
     "iopub.status.idle": "2024-10-04T15:38:15.484905Z",
     "shell.execute_reply": "2024-10-04T15:38:15.484331Z",
     "shell.execute_reply.started": "2024-10-04T15:38:11.197905Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preliminary variables ##\n",
    "\n",
    "# Specifies whether to train on GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loss for training\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Measurements\n",
    "jaccard = JaccardIndex(task='multiclass', num_classes = 2, average = 'micro').to(device)\n",
    "dice = Dice(num_classes = 2, average = 'micro').to(device)\n",
    "\n",
    "# K-Fold object for splitting dataset, randomizes batches (shuffle = True)\n",
    "splits = 5\n",
    "kf = KFold(splits, shuffle=True)\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Stopping threshold\n",
    "threshold = 0.001\n",
    "\n",
    "# Speeds up training\n",
    "num_workers = 8\n",
    "\n",
    "# Alpha for EMA\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Model and Metrics\n",
    "First we must create a baseline result to compare to. Using K-fold validation we will train 5 seperate models:\n",
    "1. Using K-fold validation we will attain 5 splits of 20% of the dataset {A,B,C,D,E}\n",
    "2. We stash away one of the splits, ex. A, for testing and train on the remaining 4 splits until we reach the stopping rule\n",
    "3. Test the train model on the test splits and save the Jaccard and Dice scores\n",
    "4. Repeat the previous steps but stash away a different split, ex. B,C,D, or E, until we have tested on all splits.\n",
    "5. Take the average of all the Dice and Jaccard scores to achieve the baseline results\n",
    "\n",
    "To optimize the model we will begin our learning rate at 0.01, use Cosine Annealing LR to decay the learning rate, and the Adam method of optimization.\n",
    "\n",
    "After the experiment is done we will save the indices of each fold and the baseline metrics to use in semi-automatic segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-04T15:38:15.485724Z",
     "iopub.status.busy": "2024-10-04T15:38:15.485563Z",
     "iopub.status.idle": "2024-10-04T19:39:34.586318Z",
     "shell.execute_reply": "2024-10-04T19:39:34.585694Z",
     "shell.execute_reply.started": "2024-10-04T15:38:15.485709Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/mateuszbuda/brain-segmentation-pytorch/zipball/master\" to /root/.cache/torch/hub/master.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:446 | Loss: 0.003184073324052569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:248 | Loss: 0.003909795533101528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:190 | Loss: 0.004509060267502299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:384 | Loss: 0.0035497269963320247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations:436 | Loss: 0.0033825317529054023\n",
      "Baseline Jaccard: 0.9213233041763307\n",
      "Baseline Dice: 0.9587881147861481\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to save each fold\n",
    "fold_name = ['A','B','C','D','E'] # To save indicies of each fold\n",
    "fold_dict = {}\n",
    "# Initialize Jaccard and Dice\n",
    "average_jaccard = 0\n",
    "average_dice = 0\n",
    "average_iterations = 0\n",
    "\n",
    "# Split dataset and determine baseline jaccard and dice\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(dataset)):\n",
    "    \n",
    "    ## Initialize the datasets ##\n",
    "    \n",
    "    # Get train loader for fold\n",
    "    train_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(train_idx),\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    # Get test loader for fold\n",
    "    test_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_idx),\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    \n",
    "    # Save indices in dictionary for future experiments\n",
    "    fold_dict.update({fold_name[fold] : [train_idx, test_idx]})\n",
    "    \n",
    "    ## Initialize the model ##\n",
    "\n",
    "    # Loading an untrained model to GPU/CPU\n",
    "    model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "        in_channels=3, out_channels=1, init_features=64, pretrained=False, trust_repo=True).to(device)\n",
    "    # We will begin our learning rate at 0.01 \n",
    "    lr = 0.01\n",
    "    # Optimizer for model\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,25)\n",
    "    \n",
    "    ## Initialize the training ##\n",
    "\n",
    "    L_MA = 1 # Moving average of loss, initialize to 0 so no division by zero error\n",
    "    L_k = 0 # Current loss\n",
    "\n",
    "    # To determine in threshold is too low\n",
    "    counter = 0\n",
    "    # Train model until stopping rule is reached\n",
    "    while(stopping_rule(L_MA, L_k, threshold) or counter < 10):\n",
    "        # Train model and compute loss\n",
    "        L_k = train_model(model, loss_fn, device, train_loader, optimizer)\n",
    "\n",
    "        # Initialization\n",
    "        if(L_MA == 0):\n",
    "            L_MA = L_k\n",
    "\n",
    "        # Find EMA of losses\n",
    "        L_MA = moving_avg(alpha, L_MA, L_k)\n",
    "        counter += 1\n",
    "    # To determine in threshold is too low\n",
    "    print(f\"Iterations:{counter} | Loss: {L_k}\")\n",
    "    # Test model on remaining splits\n",
    "    jaccard_i, dice_i = test_model(model, device, test_loader, jaccard, dice)\n",
    "    average_jaccard += jaccard_i\n",
    "    average_dice += dice_i\n",
    "    average_iterations += counter\n",
    "\n",
    "# Take average of jaccard and dice of all 5 models\n",
    "average_jaccard /= 5    \n",
    "average_dice /= 5\n",
    "average_iterations /= 5\n",
    "\n",
    "# Print results\n",
    "print(f\"Baseline Jaccard: {average_jaccard}\")\n",
    "print(f\"Baseline Dice: {average_dice}\")\n",
    "\n",
    "# Save the indicies and baseline scores into a state dict for future use\n",
    "name = 'baseline.pt'\n",
    "state = {\n",
    "    'fold_dict' : fold_dict,\n",
    "    'baseline_jaccard' : average_jaccard,\n",
    "    'baseline_dice' : average_dice,\n",
    "    'baseline_iterations' : average_iterations\n",
    "}\n",
    "torch.save(state, f = name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
