{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:06.022459Z",
     "iopub.status.busy": "2024-10-06T04:25:06.021986Z",
     "iopub.status.idle": "2024-10-06T04:25:10.414796Z",
     "shell.execute_reply": "2024-10-06T04:25:10.414132Z",
     "shell.execute_reply.started": "2024-10-06T04:25:06.022439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.1.1+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.3)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (23.2)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.1.1+cu121)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.11.7)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (69.0.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "Packages required for experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:10.416643Z",
     "iopub.status.busy": "2024-10-06T04:25:10.416045Z",
     "iopub.status.idle": "2024-10-06T04:25:12.754412Z",
     "shell.execute_reply": "2024-10-06T04:25:12.753995Z",
     "shell.execute_reply.started": "2024-10-06T04:25:10.416623Z"
    }
   },
   "outputs": [],
   "source": [
    "# General use for DL\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# To loader dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Data Measurements\n",
    "from torchmetrics import JaccardIndex\n",
    "from torchmetrics import Dice\n",
    "\n",
    "# Dataset import\n",
    "from dataset import ChestXRaysSegmentationDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopping Rule\n",
    "When this function returns a value less than 0.1 % = 0.001 then we know to stop training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.755466Z",
     "iopub.status.busy": "2024-10-06T04:25:12.754948Z",
     "iopub.status.idle": "2024-10-06T04:25:12.757791Z",
     "shell.execute_reply": "2024-10-06T04:25:12.757431Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.755447Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finds percent change between previous and current loss\n",
    "# and if it is less than threshold return false\n",
    "def stopping_rule(L_k, L_k1, threshold):\n",
    "    return abs(L_k - L_k1) / L_k > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.758974Z",
     "iopub.status.busy": "2024-10-06T04:25:12.758617Z",
     "iopub.status.idle": "2024-10-06T04:25:12.761269Z",
     "shell.execute_reply": "2024-10-06T04:25:12.760895Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.758958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculates the moving average\n",
    "def moving_avg(alpha, L_MA, L_k):\n",
    "    return alpha * L_MA + (1-alpha) * L_k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.762022Z",
     "iopub.status.busy": "2024-10-06T04:25:12.761707Z",
     "iopub.status.idle": "2024-10-06T04:25:12.765033Z",
     "shell.execute_reply": "2024-10-06T04:25:12.764685Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.762006Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, loss_fn, device, train_loader, optimizer):\n",
    "    # Initalize loss\n",
    "    average_loss = 0\n",
    "    # Train on dataset\n",
    "    model.train()\n",
    "    for batch_idx, (X,y) in enumerate(train_loader):\n",
    "        # Get batch\n",
    "        image, mask = X.to(device), y.to(device)\n",
    "        # Get results\n",
    "        output = model(image)\n",
    "        # Compute loss\n",
    "        loss = loss_fn(output, mask)\n",
    "        average_loss += loss.item()\n",
    "        # Optimize model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Return average loss\n",
    "    return average_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.765719Z",
     "iopub.status.busy": "2024-10-06T04:25:12.765457Z",
     "iopub.status.idle": "2024-10-06T04:25:12.769140Z",
     "shell.execute_reply": "2024-10-06T04:25:12.768801Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.765705Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_model(model, device, test_loader, jaccard, dice):\n",
    "    # Initalize average jaccard and dice\n",
    "    average_jaccard = 0\n",
    "    average_dice = 0\n",
    "    # Test the model\n",
    "    model.eval()\n",
    "    for batch_idx, (X,y) in enumerate(test_loader):\n",
    "        # Get batch\n",
    "        image, mask = X.to(device), y.to(device)\n",
    "        # Get results\n",
    "        output = model(image)\n",
    "        average_jaccard += jaccard(torch.where(output > 0.5, 1, 0),torch.where(mask > 0.50, 1, 0)).item()\n",
    "        average_dice += dice(torch.where(output > 0.5, 1, 0),torch.where(mask > 0.50, 1, 0)).item()\n",
    "    # Get average of dice and jaccard scores\n",
    "    average_jaccard /= len(test_loader)\n",
    "    average_dice /= len(test_loader)\n",
    "\n",
    "    # Return values\n",
    "    return average_jaccard, average_dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to partition list\n",
    "We will use this function to partition the list of train indices into eighths. If we remember the train indices consist of 80 % of the original dataset, so each partition will contain 10 % of the original dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.769760Z",
     "iopub.status.busy": "2024-10-06T04:25:12.769553Z",
     "iopub.status.idle": "2024-10-06T04:25:12.772637Z",
     "shell.execute_reply": "2024-10-06T04:25:12.772330Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.769747Z"
    }
   },
   "outputs": [],
   "source": [
    "def split_into_eights(list):\n",
    "    # Floor division of length of list\n",
    "    partition_size = len(list) // 8\n",
    "    remainder  = len(list) % 8\n",
    "    \n",
    "    # List that will store each parition\n",
    "    partitions = []\n",
    "    \n",
    "    # Partition the list, if partition is not even distrubutes \n",
    "    # remainder between beginning paritions\n",
    "    start = 0\n",
    "    for i in range(8):\n",
    "        end = start + partition_size + (1 if i < remainder and i != 0 else 0)\n",
    "        partitions.append(list[start:end])\n",
    "        start = end\n",
    "    return partitions  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "Load the baseline data and indices splits from previous stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.773398Z",
     "iopub.status.busy": "2024-10-06T04:25:12.773039Z",
     "iopub.status.idle": "2024-10-06T04:25:12.780151Z",
     "shell.execute_reply": "2024-10-06T04:25:12.779830Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.773383Z"
    }
   },
   "outputs": [],
   "source": [
    "# File name\n",
    "name = 'baseline.pt'\n",
    "baseline = torch.load(f=name)\n",
    "# Extract indices and baseline jaccard and dice scores\n",
    "all_indices = baseline['fold_dict']\n",
    "baseline_jaccard = baseline['baseline_jaccard']\n",
    "baseline_dice = baseline['baseline_dice']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.781021Z",
     "iopub.status.busy": "2024-10-06T04:25:12.780635Z",
     "iopub.status.idle": "2024-10-06T04:25:12.808656Z",
     "shell.execute_reply": "2024-10-06T04:25:12.808304Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.781007Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 704/704 [00:00<00:00, 2801508.55it/s]\n",
      "100%|██████████| 704/704 [00:00<00:00, 2713961.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Path to images and masks\n",
    "image_path = './chest_xray/images'\n",
    "mask_path = './chest_xray/masks'\n",
    "size = 256\n",
    "# Import the dataset\n",
    "dataset = ChestXRaysSegmentationDataset(image_path,mask_path,size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:12.810133Z",
     "iopub.status.busy": "2024-10-06T04:25:12.809728Z",
     "iopub.status.idle": "2024-10-06T04:25:15.294112Z",
     "shell.execute_reply": "2024-10-06T04:25:15.293559Z",
     "shell.execute_reply.started": "2024-10-06T04:25:12.810115Z"
    }
   },
   "outputs": [],
   "source": [
    "## Preliminary variables ##\n",
    "\n",
    "# Specifies whether to train on GPU or CPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Loss for training\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Measurements\n",
    "jaccard = JaccardIndex(task='multiclass', num_classes = 2, average = 'micro').to(device)\n",
    "dice = Dice(num_classes = 2, average = 'micro').to(device)\n",
    "\n",
    "# Batch Size\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# Stopping threshold\n",
    "threshold = 0.001\n",
    "\n",
    "# Speeds up training\n",
    "num_workers = 8\n",
    "\n",
    "# Alpha for EMA\n",
    "alpha = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-06T04:25:15.295228Z",
     "iopub.status.busy": "2024-10-06T04:25:15.294827Z",
     "iopub.status.idle": "2024-10-07T08:48:15.369675Z",
     "shell.execute_reply": "2024-10-07T08:48:15.369131Z",
     "shell.execute_reply.started": "2024-10-06T04:25:15.295211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "Split:A|Partition:0.1|Jaccard:0.8689605924818251|Dice:0.9295347929000854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945\n",
      "Split:A|Partition:0.2|Jaccard:0.9219139085875617|Dice:0.9592072632577684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "344\n",
      "Split:A|Partition:0.3|Jaccard:0.938945902718438|Dice:0.9684959914949205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692\n",
      "Split:A|Partition:0.4|Jaccard:0.9364785022205777|Dice:0.9671646356582642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "Split:A|Partition:0.5|Jaccard:0.9423675338427225|Dice:0.9703133967187669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612\n",
      "Split:A|Partition:0.6|Jaccard:0.9524468845791287|Dice:0.9756412837240431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "406\n",
      "Split:A|Partition:0.7|Jaccard:0.9588484764099121|Dice:0.9789837797482809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "593\n",
      "Split:B|Partition:0.1|Jaccard:0.8715821570820279|Dice:0.9308970438109504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364\n",
      "Split:B|Partition:0.2|Jaccard:0.9259426660007901|Dice:0.9614260064231025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "Split:B|Partition:0.3|Jaccard:0.9342492487695482|Dice:0.9659856160481771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n",
      "Split:B|Partition:0.4|Jaccard:0.9233459234237671|Dice:0.960111571682824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n",
      "Split:B|Partition:0.5|Jaccard:0.9450196160210503|Dice:0.971705953280131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "Split:B|Partition:0.6|Jaccard:0.9482164780298868|Dice:0.9734048247337341\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "Split:B|Partition:0.7|Jaccard:0.9581540889210172|Dice:0.978609475824568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248\n",
      "Split:C|Partition:0.1|Jaccard:0.8733338382509019|Dice:0.932131807009379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2040\n",
      "Split:C|Partition:0.2|Jaccard:0.9353870815700955|Dice:0.9665791193644205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337\n",
      "Split:C|Partition:0.3|Jaccard:0.9347047209739685|Dice:0.9661864572101169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278\n",
      "Split:C|Partition:0.4|Jaccard:0.9419408043225607|Dice:0.9700709184010824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n",
      "Split:C|Partition:0.6|Jaccard:0.941914955774943|Dice:0.9700656533241272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n",
      "Split:C|Partition:0.7|Jaccard:0.9586775369114346|Dice:0.9788919422361586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "Split:D|Partition:0.1|Jaccard:0.8724015752474467|Dice:0.9316100676854452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "Split:D|Partition:0.2|Jaccard:0.9172800845570035|Dice:0.9567976792653402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n",
      "Split:D|Partition:0.3|Jaccard:0.9223662681049771|Dice:0.9595598777135214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n",
      "Split:D|Partition:0.4|Jaccard:0.9278005162874857|Dice:0.9624797900517782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n",
      "Split:D|Partition:0.5|Jaccard:0.942874981297387|Dice:0.970564497841729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "331\n",
      "Split:D|Partition:0.6|Jaccard:0.950145939985911|Dice:0.9744140108426412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n",
      "Split:D|Partition:0.7|Jaccard:0.9586351977454292|Dice:0.9788732992278205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n",
      "Split:E|Partition:0.1|Jaccard:0.8861263460583158|Dice:0.9393250147501627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n",
      "Split:E|Partition:0.2|Jaccard:0.9101462033059862|Dice:0.9528409441312155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "434\n",
      "Split:E|Partition:0.3|Jaccard:0.9410359329647489|Dice:0.9695913195610046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n",
      "Split:E|Partition:0.4|Jaccard:0.9479281769858466|Dice:0.9732448061307272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "Split:E|Partition:0.5|Jaccard:0.9438536630736457|Dice:0.9711029595798917\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "286\n",
      "Split:E|Partition:0.6|Jaccard:0.9448421663708158|Dice:0.9716153343518575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "541\n",
      "Split:E|Partition:0.7|Jaccard:0.9597532219356961|Dice:0.9794496960110135\n"
     ]
    }
   ],
   "source": [
    "splits = ['A', 'B', 'C', 'D', 'E']\n",
    "partitions = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7]\n",
    "# Cycle through all test splits\n",
    "for split in splits:\n",
    "    # Get indices of test and train points in dataset\n",
    "    indices = all_indices[split]\n",
    "    train_indices = indices[0]\n",
    "    test_indices = indices[1]\n",
    "    # Split train indices into eighths\n",
    "    partition_indices = split_into_eights(train_indices)\n",
    "    # Create test dataloader for fold\n",
    "    test_loader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        sampler=torch.utils.data.SubsetRandomSampler(test_indices),\n",
    "        num_workers = num_workers\n",
    "    )\n",
    "    # Cycle through all partition lengths of training data (0.1,0.2,...,0.7)\n",
    "    for i in range(1, len(partition_indices)):\n",
    "        \n",
    "        # Initialize Jaccard and Dice\n",
    "        average_jaccard = 0\n",
    "        average_dice = 0\n",
    "                \n",
    "        train_indices_i = np.hstack(partition_indices[0:i])\n",
    "        remaining_indices = np.hstack(partition_indices[i:])\n",
    "        \n",
    "        # Create a train dataloader for partition\n",
    "        train_loader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size = BATCH_SIZE,\n",
    "            sampler=torch.utils.data.SubsetRandomSampler(train_indices_i),\n",
    "            num_workers = num_workers\n",
    "        )\n",
    "        \n",
    "        ## Initialize the model ##\n",
    "\n",
    "        # Loading an untrained model to GPU/CPU\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "            in_channels=3, out_channels=1, init_features=64, pretrained=False, trust_repo=True).to(device)\n",
    "        # We will begin our learning rate at 0.01 \n",
    "        lr = 0.01\n",
    "        # Optimizer for model\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,25)\n",
    "        \n",
    "        ## Initialize the training ##\n",
    "    \n",
    "        # Initialize previous and current loss for stopping rule\n",
    "        \n",
    "        L_MA = 1 # Moving average of loss\n",
    "        L_k = 0 # Current loss\n",
    "        \n",
    "        # To determine in threshold is too low\n",
    "        counter = 0\n",
    "        # Train model until stopping rule is reached\n",
    "        while(stopping_rule(L_MA, L_k, threshold) or counter < 10):\n",
    "            # Train model and compute loss\n",
    "            L_k = train_model(model, loss_fn, device, train_loader, optimizer)\n",
    "            \n",
    "            # Initialization of EMA\n",
    "            if(L_MA == 0):\n",
    "                L_MA = L_k\n",
    "            \n",
    "            # Find EMA of losses\n",
    "            L_MA = moving_avg(alpha, L_MA, L_k)\n",
    "            counter += 1\n",
    "               \n",
    "        # To determine in threshold is too low\n",
    "        print(counter)\n",
    "        # Test model on test split\n",
    "        jaccard_score, dice_score = test_model(model, device, test_loader, jaccard, dice)\n",
    "        print(f'Split:{split}|Partition:{partitions[i]}|Jaccard:{jaccard_score}|Dice:{dice_score}')\n",
    "        # Save model, scores, and indices for next step\n",
    "        name = f'./model/Split:{split}|Partition:{partitions[i]}|New'\n",
    "        state = {\n",
    "            'train_indices' : train_indices_i,\n",
    "            'remaining_indices' : remaining_indices,\n",
    "            'test_indices' : test_indices,\n",
    "            'jaccard_score' : jaccard_score,\n",
    "            'dice_score' : dice_score,\n",
    "            'num_iterations' : counter,\n",
    "            'state_dict' : model.state_dict(),\n",
    "        }\n",
    "        torch.save(state, f=name)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
